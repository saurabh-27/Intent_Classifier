{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/saurabh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/saurabh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/saurabh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-20de31eb966d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wordnet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "nltk.download(\"wordnet\")\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Intent                     Utterance\n",
      "0  who_brand_ambd              who is brandambd\n",
      "1     future_cars                   future cars\n",
      "2          ismart                    smart cars\n",
      "3        bmw_fuel       tell me about fuel type\n",
      "4  who_brand_ambd  who is your brand ambassador\n"
     ]
    }
   ],
   "source": [
    "df = load_data(\"/home/saurabh/Desktop/bmw_training_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconvenience        107\n",
      "bmw_book              60\n",
      "bmw_specification     50\n",
      "bmwfeature            43\n",
      "nextcarlaunch         35\n",
      "Name: Intent, dtype: int64\n",
      "how-is-your-car-different    2\n",
      "who-is-pretty                2\n",
      "are-you-human                2\n",
      "flip-a-coin                  2\n",
      "delivery                     1\n",
      "Name: Intent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Intent'].value_counts().head())\n",
    "print(df['Intent'].value_counts().tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    cleaned_text = \" \".join(tokenized_text)\n",
    "    cleaned_text = \" \".join((wn.lemmatize(word)) for word in cleaned_text.split() if word not in stop_words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_text=df['Utterance'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           brandambd\n",
      "1          future car\n",
      "2           smart car\n",
      "3      tell fuel type\n",
      "4    brand ambassador\n",
      "Name: Utterance, dtype: object\n",
      "1451\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_text[:5])\n",
    "print(len(cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intent</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>intent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who_brand_ambd</td>\n",
       "      <td>who is brandambd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>future_cars</td>\n",
       "      <td>future cars</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ismart</td>\n",
       "      <td>smart cars</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bmw_fuel</td>\n",
       "      <td>tell me about fuel type</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who_brand_ambd</td>\n",
       "      <td>who is your brand ambassador</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Intent                     Utterance  intent_id\n",
       "0  who_brand_ambd              who is brandambd          0\n",
       "1     future_cars                   future cars          1\n",
       "2          ismart                    smart cars          2\n",
       "3        bmw_fuel       tell me about fuel type          3\n",
       "4  who_brand_ambd  who is your brand ambassador          0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"intent_id\"] = df[\"Intent\"].factorize()[0]\n",
    "intent_id_df = df[['Intent', 'intent_id']].drop_duplicates().sort_values('intent_id')\n",
    "id_to_intent = dict(intent_id_df[['intent_id', 'Intent']].values)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'who_brand_ambd', 1: 'future_cars', 2: 'ismart', 3: 'bmw_fuel', 4: 'bmw_price', 5: 'bmw_city', 6: 'brand_ambd_common', 7: 'brand_ambd_to_india', 8: 'bmw_book', 9: 'applydealership', 10: 'bmwinternet', 11: 'about_bmwcc', 12: 'about', 13: 'bmwwhat', 14: 'bmw_launch', 15: 'bmw_india', 16: 'bmwfeature', 17: 'likeness', 18: 'lol', 19: 'competitor', 20: 'you-are-dumb', 21: 'careers', 22: 'on-road-price', 23: 'mentoring', 24: 'what_is_your_name', 25: 'say-something-funny', 26: 'bmw_buy', 27: 'how-do-i-look', 28: 'can-you-hear-me', 29: 'bmw_chinese_brand', 30: 'maximum-demand', 31: 'chinese-parts', 32: 'you-are-awesome', 33: 'access_to_openfiles', 34: 'fine', 35: 'are-you-smart', 36: 'your-family', 37: 'shut-up', 38: 'boring_tired', 39: 'protection-plans', 40: 'nextcarlaunch', 41: 'demand-from-diesel-variant', 42: 'bmw_specification', 43: 'produced-or-assembled', 44: 'user_details', 45: 'who-made-you', 46: 'are-you-hungry', 47: 'warranty', 48: 'csr', 49: 'angry', 50: 'main_menu', 51: 'bookings-received', 52: 'ask-me-anything', 53: 'waiting-period', 54: 'manufacturing_plant', 55: 'doing-work', 56: 'download', 57: 'bmw-service-details', 58: 'contact', 59: 'competition-from-kia-seltos', 60: 'you-are-fired', 61: 'hungry', 62: 'no-sense', 63: 'comments-about-recent-slowdown', 64: 'favorites', 65: 'chinesebrand', 66: 'compare-variants', 67: 'each_partner_contributing', 68: 'waitcompetitor', 69: 'dreams_fly_cooking', 70: 'assured-buyback', 71: 'anyone-there', 72: 'gender-questions', 73: 'whoisboss', 74: 'maximum_amount_-given_by_bmw', 75: 'how_old_are_you', 76: 'role_of_saic', 77: 'iit_-5-_startups', 78: 'total_outlay_earmarked', 79: 'bs_vi-version', 80: 'rule-the-world', 81: 'i-am-happy', 82: 'do-you-love-me', 83: 'bmwmanufacturing', 84: 'i-am-bored', 85: 'program_faredout', 86: 'solutions_patented', 87: 'models-for-next-2-years', 88: 'comliment', 89: 'tired-sleepy', 90: 'flip-a-coin', 91: 'meaning-of-life', 92: 'are-you-human', 93: 'bmw_reopenings', 94: 'test_drive', 95: 'road-side-protection', 96: 'i-feel-sad', 97: 'doing', 98: 'where-are-you', 99: 'sales-target', 100: 'whats-the-range', 101: 'british-chinese', 102: 'who-is-pretty', 103: 'inconvenience', 104: 'nothing', 105: 'i-miss-you', 106: 'in-how-many-cities-will-it-be-sold-in', 107: 'download-test', 108: 'if-i-change-my-preferred-booked-variant-or-color-choice', 109: 'if-i-want-to-book-bmw-through-showroom-walk-in', 110: 'how-is-your-car-different', 111: 'are-there-any-issues-in-the-bmw', 112: 'delivery', 113: 'off-road', 114: 'automatic-variant-of-bmw', 115: 'will-the-data-be-sent-to-china', 116: 'get-delivery', 117: 'points', 118: 'changemaker', 119: 'reviewing', 120: 'event', 121: 'appreciation', 122: 'return-policy', 123: 'site-not-working', 124: 'safety', 125: 'ev-safety', 126: 'battery-replace-cost', 127: 'components', 128: 'weight', 129: 'itsahumanthing', 130: 'sponsorship', 131: 'powertrain', 132: 'single-charge', 133: 'battery-life', 134: 'replace-battery', 135: 'car-service', 136: 'drive-her-back', 137: 'exchange-policy', 138: 'ev-warranty', 139: 'apple-carplay', 140: 'press-release', 141: 'home-charger', 142: 'standard-equipment', 143: 'more-info'}\n"
     ]
    }
   ],
   "source": [
    "print(id_to_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-4e3b24810ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.5/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.5/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.5/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.5/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.5/site-packages/keras/backend/load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp35-cp35m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2 MB 9.4 kB/s \n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 389 kB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/saurabh/anaconda3/lib/python3.5/site-packages (from tensorflow) (0.29.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.12.0-cp35-cp35m-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.4 MB/s \n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from tensorflow) (1.1.2)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 2.8 MB/s \n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 295 kB/s \n",
      "\u001b[?25hCollecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.1-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 2.2 MB/s \n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.29.0-cp35-cp35m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 169 kB/s \n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 1.7 MB/s \n",
      "\u001b[?25hCollecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Downloading scipy-1.4.1-cp35-cp35m-manylinux1_x86_64.whl (26.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0 MB 94 kB/s \n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp35-cp35m-manylinux1_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 36 kB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/saurabh/anaconda3/lib/python3.5/site-packages (from protobuf>=3.8.0->tensorflow) (40.7.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "\u001b[K     |████████████████████████████████| 777 kB 2.9 MB/s \n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.15.0-py2.py3-none-any.whl (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 1.7 MB/s \n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 4.8 MB/s \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 1.2 MB/s \n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.21.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 2.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/saurabh/anaconda3/lib/python3.5/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2018.11.29)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.1.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.2.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 5.3 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, wrapt, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=5680 sha256=8ded3e28b06ecb15aaaeb1f0b54f2cf2d4666fe7d8e96736eed6f2f16fffe08a\n",
      "  Stored in directory: /home/saurabh/.cache/pip/wheels/91/0e/11/1f1321dce76e9c542907008e4a94ff79f8bf525a3fa32b09f3\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp35-cp35m-linux_x86_64.whl size=68080 sha256=ab6106327a108220e4119e9cee556d1848f03dc1a3368834518d058cd741522f\n",
      "  Stored in directory: /home/saurabh/.cache/pip/wheels/b8/2d/88/2fb764a7404287819251978fbfe4cacf9b4d8a1d41fce5dd97\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=119398 sha256=b0948d78bc9d16fb371786c6077cd5dfff45517e8503ada3fab9d3b28d35f40f\n",
      "  Stored in directory: /home/saurabh/.cache/pip/wheels/00/c0/fe/b499a8663e1697aa205f83a8b15a53a29dc4b9831643b0064b\n",
      "Successfully built termcolor wrapt absl-py\n",
      "\u001b[31mERROR: tensorboard 2.2.1 has requirement setuptools>=41.0.0, but you'll have setuptools 40.7.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pyasn1-modules 0.2.8 has requirement pyasn1<0.5.0,>=0.4.6, but you'll have pyasn1 0.1.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: google-pasta, protobuf, termcolor, wrapt, astunparse, tensorflow-estimator, opt-einsum, tensorboard-plugin-wit, cachetools, rsa, pyasn1-modules, google-auth, werkzeug, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, grpcio, tensorboard, scipy, gast, h5py, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.10.6\n",
      "\u001b[31mERROR: Cannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/saurabh/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras in /home/saurabh/anaconda3/lib/python3.5/site-packages (2.3.1)\n",
      "Requirement already satisfied: h5py in /home/saurabh/anaconda3/lib/python3.5/site-packages (from Keras) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from Keras) (1.1.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from Keras) (1.2.0)\n",
      "Requirement already satisfied: pyyaml in /home/saurabh/anaconda3/lib/python3.5/site-packages (from Keras) (3.12)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from Keras) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from Keras) (1.16.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/saurabh/anaconda3/lib/python3.5/site-packages (from Keras) (1.0.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/saurabh/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
